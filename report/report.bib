-*- eval: (visual-line-mode 1) -*-

@ARTICLE{He09, 
author={Lihan He and Carin, L.}, 
journal={Signal Processing, IEEE Transactions on}, 
title={Exploiting Structure in Wavelet-Based Bayesian Compressive Sensing}, 
year=2009, 
volume=57, 
number=9, 
pages={3488-3497}, 
abstract={Bayesian compressive sensing (CS) is considered for signals and images that are sparse in a wavelet basis. The statistical structure of the wavelet coefficients is exploited explicitly in the proposed model, and, therefore, this framework goes beyond simply assuming that the data are compressible in a wavelet basis. The structure exploited within the wavelet coefficients is consistent with that used in wavelet-based compression algorithms. A hierarchical Bayesian model is constituted, with efficient inference via Markov chain Monte Carlo (MCMC) sampling. The algorithm is fully developed and demonstrated using several natural images, with performance comparisons to many state-of-the-art compressive-sensing inversion algorithms.}, 
keywords={Markov processes;Monte Carlo methods;belief networks;data compression;image coding;image sampling;inference mechanisms;wavelet transforms;Markov chain Monte Carlo sampling;compressive sensing inversion algorithms;statistical structure;wavelet coefficients;wavelet-based Bayesian compressive sensing;Bayesian signal processing;compression;sparseness;wavelets}, 
doi={10.1109/TSP.2009.2022003}, 
ISSN={1053-587X},}

@INPROCEEDINGS{Yang10,
author={Shunliao Yang and Zhengbing Zhang and Hong Du and Zhenghua Xia and Hongying Qin}, 
booktitle={Intelligence Information Processing and Trusted Computing (IPTC), 2010 International Symposium on}, 
title={The Compressive Sensing Based on Biorthogonal Wavelet Basis}, 
year=2010, 
pages={479-482}, 
abstract={Compressive Sensing is a new theory to simultaneous sensing and compression. It enables a potentially large reduction in the sampling and computation costs for signals based on them having sparse representation in some basis. Many wavelet transformations were used in CS, such as Haar, db4, db6 and db8 wavelets. We test several wavelet basis from Daubechies and Biorthogonal family using Bayesian wavelet-tree structured CS. The Error Ratio between the original coefficient and the reconstructed coefficient, the PSNR of the original image and reconstructed image, and the Elapsed Time were used as the measurement indexes. Two images, Indor3 and Lena are used in experimental. The results indicate that the Biorthogonal wavelet family, from which bior2.8 and bior3.5 are used in this paper, can get the better results than other wavelet basis.}, 
keywords={Bayes methods;data compression;data reduction;image reconstruction;trees (mathematics);wavelet transforms;Bayesian wavelet-tree;Daubechies family;biorthogonal wavelet basis;compressive sensing;data reduction;error ratio;image reconstruction;sparse representation;wavelet transformations;Biomedical imaging;Compressed sensing;Erbium;Image reconstruction;PSNR;Wavelet coefficients;Biorthogonal wavelet;Daubechies wavelet;Haar;compressive sensing;sparsenes}, 
doi={10.1109/IPTC.2010.146},}

@ARTICLE{Donoho06,
author={Donoho, D.L.}, 
journal={Information Theory, IEEE Transactions on}, 
title={Compressed sensing}, 
year={2006}, 
volume={52}, 
number={4}, 
pages={1289-1306}, 
keywords={convex programming;data compression;image coding;image reconstruction;image sampling;image sensors;sparse matrices;transform coding;Euclidean space;convex optimization;general linear functional measurement;image reconstruction;nonadaptive nonpixel sampling;sensing compression;signal processing;sparse representation;transform coding;Compressed sensing;Data mining;Digital images;Image coding;Image reconstruction;Pixel;Signal processing;Size measurement;Transform coding;Vectors;Adaptive sampling;Basis Pursuit;Gel'fand;Quotient-of-a-Subspace theorem;almost-spherical sections of Banach spaces;eigenvalues of random matrices;information-based complexity;integrated sensing and processing;minimum;optimal recovery;sparse solution of linear equations},
abstract={},
doi={10.1109/TIT.2006.871582}, 
ISSN={0018-9448},}

@ARTICLE{Candes08, 
author={Candes, E.J. and Wakin, M.B.}, 
journal={Signal Processing Magazine, IEEE}, 
title={An Introduction To Compressive Sampling}, 
year=2008, 
volume=25, 
number=2, 
pages={21-30}, 
abstract={Conventional approaches to sampling signals or images follow Shannon's theorem: the sampling rate must be at least twice the maximum frequency present in the signal (Nyquist rate). In the field of data conversion, standard analog-to-digital converter (ADC) technology implements the usual quantized Shannon representation - the signal is uniformly sampled at or above the Nyquist rate. This article surveys the theory of compressive sampling, also known as compressed sensing or CS, a novel sensing/sampling paradigm that goes against the common wisdom in data acquisition. CS theory asserts that one can recover certain signals and images from far fewer samples or measurements than traditional methods use.}, 
keywords={data acquisition;image processing;signal processing equipment;signal sampling;Relatively few wavelet;compressed sensing;compressive sampling;data acquisition;image recovery;sampling paradigm;sensing paradigm;signal recovery;Biomedical imaging;Data acquisition;Frequency;Image coding;Image sampling;Protocols;Receivers;Sampling methods;Signal processing;Signal sampling}, 
doi={10.1109/MSP.2007.914731}, 
ISSN={1053-5888},}

@ARTICLE{Candes07,
author={Emmanuel CandÃ¨s and Justin Romberg},
title={Sparsity and incoherence in compressive sampling},
journal={Inverse Problems},
volume=23,
number=3,
pages=969,
year=2007,
abstract={We consider the problem of reconstructing a sparse signal $x^0\in\R^n$ from a limited number of linear measurements. Given $m$ randomly selected samples of $U x^0$, where $U$ is an orthonormal matrix, we show that $\ell_1$ minimization recovers $x^0$ exactly when the number of measurements exceeds \[ m\geq \mathrm{Const}\cdot\mu^2(U)\cdot S\cdot\log n, \] where $S$ is the number of nonzero components in $x^0$, and $\mu$ is the largest entry in $U$ properly normalized: $\mu(U) = \sqrt{n} \cdot \max_{k,j} |U_{k,j}|$. The smaller $\mu$, the fewer samples needed. \\ The result holds for ``most'' sparse signals $x^0$ supported on a fixed (but arbitrary) set $T$. Given $T$, if the sign of $x^0$ for each nonzero entry on $T$ and the observed values of $Ux^0$ are drawn at random, the signal is recovered with overwhelming probability. Moreover, there is a sense in which this is nearly optimal since any method succeeding with the same probability would require just about this many samples.},
doi={10.1088/0266-5611/23/3/008},}

@ARTICLE{Candes06,
author={Candes, E.J. and Romberg, J. and Tao, T.}, 
journal={Information Theory, IEEE Transactions on}, 
title={Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information}, 
year=2006, 
volume=52, 
number=2, 
pages={489-509}, 
abstract={This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal $f \in \C^N$ and a randomly chosen set of frequencies $\Omega$ of mean size $\tau N$. Is it possible to reconstruct $f$ from the partial knowledge of its Fourier coefficients on the set $\Omega$? \\ A typical result of this paper is as follows: for each $M > 0$, suppose that $f$ obeys $$ # \{t, f(t) \neq 0 \} \le \alpha(M) \cdot (\log N)^{-1} \cdot # \Omega, $$ then with probability at least $1-O(N^{-M})$, $f$ can be reconstructed exactly as the solution to the $\ell_1$ minimization problem $$ \min_g \sum_{t = 0}^{N-1} |g(t)|, \quad \text{s.t.} \hat g(\omega) = \hat f(\omega) \text{for all} \omega \in \Omega. $$ In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for $\alpha$ which depends on the desired probability of success; except for the logarithmic factor, the condition on the size of the support is sharp. \\ The methodology extends to a variety of other setups and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one or two-dimensional) object from incomplete frequency samples--provided that the number of jumps (discontinuities) obeys the condition above--by minimizing other convex functionals such as the total-variation of $f$.}, 
keywords={Fourier analysis;convex programming;image reconstruction;image sampling;indeterminancy;linear programming;minimisation;piecewise constant techniques;probability;signal reconstruction;signal sampling;sparse matrices;Fourier coefficient;convex optimization;discrete-time signal;image reconstruction;incomplete frequency information;linear programming;minimization problem;nonlinear sampling theorem;piecewise constant object;probability value;robust uncertainty principle;signal reconstruction;sparse random matrix;trigonometric expansion;Biomedical imaging;Frequency;Image reconstruction;Linear programming;Mathematics;Robustness;Sampling methods;Signal processing;Signal reconstruction;Uncertainty;Convex optimization;duality in optimization;free probability;image reconstruction;linear programming;random matrices;sparsity;total-variation minimization;trigonometric expansions;uncertainty principle}, 
doi={10.1109/TIT.2005.862083}, 
ISSN={0018-9448},}

@INCOLLECTION{Fornasier11,
year=2011,
isbn={978-0-387-92919-4},
booktitle={Handbook of Mathematical Methods in Imaging},
editor={Scherzer, Otmar},
doi={10.1007/978-0-387-92920-0_6},
title={Compressive Sensing},
publisher={Springer New York},
author={Fornasier, Massimo and Rauhut, Holger},
pages={187-228},}

@ARTICLE{Ji08,
author={Shihao Ji and Ya Xue and Carin, L.}, 
journal={Signal Processing, IEEE Transactions on}, 
title={Bayesian Compressive Sensing}, 
year=2008, 
volume=56, 
number=6, 
pages={2346-2356}, 
abstract={The data of interest are assumed to be represented as N-dimensional real vectors, and these vectors are compressible in some linear basis B, implying that the signal can be reconstructed accurately using only a small number M Lt N of basis-function coefficients associated with B. Compressive sensing is a framework whereby one does not measure one of the aforementioned N-dimensional signals directly, but rather a set of related measurements, with the new measurements a linear combination of the original underlying N-dimensional signal. The number of required compressive-sensing measurements is typically much smaller than N, offering the potential to simplify the sensing system. Let f denote the unknown underlying N-dimensional signal, and g a vector of compressive-sensing measurements, then one may approximate f accurately by utilizing knowledge of the (under-determined) linear relationship between f and g, in addition to knowledge of the fact that f is compressible in B. In this paper we employ a Bayesian formalism for estimating the underlying signal f based on compressive-sensing measurements g. The proposed framework has the following properties: i) in addition to estimating the underlying signal f, "error bars" are also estimated, these giving a measure of confidence in the inverted signal; ii) using knowledge of the error bars, a principled means is provided for determining when a sufficient number of compressive-sensing measurements have been performed; iii) this setting lends itself naturally to a framework whereby the compressive sensing measurements are optimized adaptively and hence not determined randomly; and iv) the framework accounts for additive noise in the compressive-sensing measurements and provides an estimate of the noise variance. In this paper we present the underlying theory, an associated algorithm, example results, and provide comparisons to other compressive-sensing inversion algorithms in the literature.}, 
keywords={Bayes methods;estimation theory;noise;signal reconstruction;signal representation;Bayesian compressive sensing framework;N-dimensional signal representation;noise variance;signal estimation;signal reconstruction;Additive noise;Bars;Bayesian methods;Design for experiments;Discrete wavelet transforms;Machine learning;Noise measurement;Performance evaluation;Transform coding;Vectors;Adaptive compressive sensing;Bayesian model selection;compressive sensing (CS);experimental design;relevance vector machine (RVM);sparse Bayesian learning}, 
doi={10.1109/TSP.2007.914345}, 
ISSN={1053-587X},}

@Book{Mackay03,
 author = {MacKay, David},
 title = {Information theory, inference, and learning algorithms},
 publisher = {Cambridge University Press},
 year = {2003},
 address = {Cambridge, UK New York},
 isbn = {9780521642989},
 abstract={Information theory and inference, taught together in this exciting textbook, lie at the heart of many important areas of modern technology - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics and cryptography. The book introduces theory in tandem with applications. Information theory is taught alongside practical communication systems such as arithmetic coding for data compression and sparse-graph codes for error-correction. Inference techniques, including message-passing algorithms, Monte Carlo methods and variational approximations, are developed alongside applications to clustering, convolutional codes, independent component analysis, and neural networks. Uniquely, the book covers state-of-the-art error-correcting codes, including low-density-parity-check codes, turbo codes, and digital fountain codes - the twenty-first-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning, and for undergraduate or graduate courses. It also provides an unparalleled entry point for professionals in areas as diverse as computational biology, financial engineering and machine learning.},
 }

@ARTICLE{Candes06b, 
author={Candes, E.J. and Tao, T.}, 
journal={Information Theory, IEEE Transactions on}, 
title={Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?}, 
year=2006, 
month={Dec}, 
volume=52, 
number=12, 
pages={5406-5425}, 
abstract={Suppose we are given a vector f in a class FsubeRopfN , e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr2) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|(n)lesRmiddotn-1p/, where R>0 and p>0. Suppose that we take measurements yk=langf# ,Xkrang,k=1,...,K, where the Xk are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0<p<1 and with overwhelming probability, our reconstruction ft, defined as the solution to the constraints yk=langf# ,Xkrang with minimal lscr1 norm, obeys parf-f#parlscr2lesCp middotRmiddot(K/logN)-r, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed}, 
keywords={encoding;linear programming;signal reconstruction;N-dimensional Gaussian vector;linear measurement;linear program;random projection;signal reconstruction;signal recovery;universal encoding strategy;Concrete;Digital images;Encoding;Geometry;Image coding;Image reconstruction;Linear programming;Mathematics;Measurement standards;Vectors;Concentration of measure;convex optimization;duality in optimization;linear programming;random matrices;random projections;signal recovery;singular values of random matrices;sparsity;trigonometric expansions;uncertainty principle}, 
doi={10.1109/TIT.2006.885507}, 
ISSN={0018-9448},}

@article{Baraniuk06,
  title={The Johnson-Lindenstrauss lemma meets compressed sensing},
  author={Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
  journal={Submitted manuscript, June},
  year=2006
}

@ARTICLE{Donoho01, 
author={Donoho, D.L. and Huo, X.}, 
journal={Information Theory, IEEE Transactions on}, 
title={Uncertainty principles and ideal atomic decomposition}, 
year={2001}, 
month={Nov}, 
volume={47}, 
number={7}, 
pages={2845-2862}, 
abstract={Suppose a discrete-time signal S(t), 0â©½t<N, is a superposition of atoms taken from a combined time-frequency dictionary made of spike sequences 1{t=Ï} and sinusoids exp{2Ïiwt/N}/âN. Can one recover, from knowledge of S alone, the precise collection of atoms going to make up S? Because every discrete-time signal can be represented as a superposition of spikes alone, or as a superposition of sinusoids alone, there is no unique way of writing S as a sum of spikes and sinusoids in general. We prove that if S is representable as a highly sparse superposition of atoms from this time-frequency dictionary, then there is only one such highly sparse representation of S, and it can be obtained by solving the convex optimization problem of minimizing the l1 norm of the coefficients among all decompositions. Here âhighly sparseâ means that Nt+Nw<âN/2 where Nt is the number of time atoms, Nw is the number of frequency atoms, and N is the length of the discrete-time signal. Underlying this result is a general l1 uncertainty principle which says that if two bases are mutually incoherent, no nonzero signal can have a sparse representation in both bases simultaneously. For the above setting, the bases are sinusoids and spikes, and mutual incoherence is measured in terms of the largest inner product between different basis elements. The uncertainty principle holds for a variety of interesting basis pairs, not just sinusoids and spikes. The results have idealized applications to band-limited approximation with gross errors, to error-correcting encryption, and to separation of uncoordinated sources. Related phenomena hold for functions of a real variable, with basis pairs such as sinusoids and wavelets, and for functions of two variables, with basis pairs such as wavelets and ridgelets. In these settings, if a function f is representable by a sufficiently sparse superposition of terms taken from both bases, then there is only one such sparse representation; it may be obtained by minimum l1 norm atomic decomposition. The condition âsufficiently sparseâ becomes a multiscale condition; for example, that the number of wavelets at level j plus the number of sinusoids in the jth dyadic frequency band are together less than a constant times 2j/2 }, 
keywords={indeterminancy;signal representation;time-frequency analysis;wavelet transforms;band-limited approximation;convex optimization problem;discrete-time signal length;discrete-time signal representation;dyadic frequency band;error-correcting encryption;ideal atomic decomposition;multiscale condition;mutual incoherence;ridgelets;sinusoids;sparse representation;spike sequences;time-frequency dictionary;uncertainty principles;uncoordinated source separation;wavelets;Cryptography;Dictionaries;Frequency;Harmonic analysis;Matching pursuit algorithms;Signal analysis;Signal representations;Uncertainty;Wavelet packets;Writing}, 
doi={10.1109/18.959265}, 
ISSN={0018-9448},}

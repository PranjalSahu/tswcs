-*- eval: (visual-line-mode 1) -*-

@ARTICLE{He09, 
author={Lihan He and Carin, L.}, 
journal={Signal Processing, IEEE Transactions on}, 
title={Exploiting Structure in Wavelet-Based Bayesian Compressive Sensing}, 
year=2009, 
volume=57, 
number=9, 
pages={3488-3497}, 
abstract={Bayesian compressive sensing (CS) is considered for signals and images that are sparse in a wavelet basis. The statistical structure of the wavelet coefficients is exploited explicitly in the proposed model, and, therefore, this framework goes beyond simply assuming that the data are compressible in a wavelet basis. The structure exploited within the wavelet coefficients is consistent with that used in wavelet-based compression algorithms. A hierarchical Bayesian model is constituted, with efficient inference via Markov chain Monte Carlo (MCMC) sampling. The algorithm is fully developed and demonstrated using several natural images, with performance comparisons to many state-of-the-art compressive-sensing inversion algorithms.}, 
keywords={Markov processes;Monte Carlo methods;belief networks;data compression;image coding;image sampling;inference mechanisms;wavelet transforms;Markov chain Monte Carlo sampling;compressive sensing inversion algorithms;statistical structure;wavelet coefficients;wavelet-based Bayesian compressive sensing;Bayesian signal processing;compression;sparseness;wavelets}, 
doi={10.1109/TSP.2009.2022003}, 
ISSN={1053-587X},}

@INPROCEEDINGS{Yang10,
author={Shunliao Yang and Zhengbing Zhang and Hong Du and Zhenghua Xia and Hongying Qin}, 
booktitle={Intelligence Information Processing and Trusted Computing (IPTC), 2010 International Symposium on}, 
title={The Compressive Sensing Based on Biorthogonal Wavelet Basis}, 
year=2010, 
pages={479-482}, 
abstract={Compressive Sensing is a new theory to simultaneous sensing and compression. It enables a potentially large reduction in the sampling and computation costs for signals based on them having sparse representation in some basis. Many wavelet transformations were used in CS, such as Haar, db4, db6 and db8 wavelets. We test several wavelet basis from Daubechies and Biorthogonal family using Bayesian wavelet-tree structured CS. The Error Ratio between the original coefficient and the reconstructed coefficient, the PSNR of the original image and reconstructed image, and the Elapsed Time were used as the measurement indexes. Two images, Indor3 and Lena are used in experimental. The results indicate that the Biorthogonal wavelet family, from which bior2.8 and bior3.5 are used in this paper, can get the better results than other wavelet basis.}, 
keywords={Bayes methods;data compression;data reduction;image reconstruction;trees (mathematics);wavelet transforms;Bayesian wavelet-tree;Daubechies family;biorthogonal wavelet basis;compressive sensing;data reduction;error ratio;image reconstruction;sparse representation;wavelet transformations;Biomedical imaging;Compressed sensing;Erbium;Image reconstruction;PSNR;Wavelet coefficients;Biorthogonal wavelet;Daubechies wavelet;Haar;compressive sensing;sparsenes}, 
doi={10.1109/IPTC.2010.146},}

@ARTICLE{Donoho06,
author={Donoho, D.L.}, 
journal={Information Theory, IEEE Transactions on}, 
title={Compressed sensing}, 
year={2006}, 
volume={52}, 
number={4}, 
pages={1289-1306}, 
keywords={convex programming;data compression;image coding;image reconstruction;image sampling;image sensors;sparse matrices;transform coding;Euclidean space;convex optimization;general linear functional measurement;image reconstruction;nonadaptive nonpixel sampling;sensing compression;signal processing;sparse representation;transform coding;Compressed sensing;Data mining;Digital images;Image coding;Image reconstruction;Pixel;Signal processing;Size measurement;Transform coding;Vectors;Adaptive sampling;Basis Pursuit;Gel'fand;Quotient-of-a-Subspace theorem;almost-spherical sections of Banach spaces;eigenvalues of random matrices;information-based complexity;integrated sensing and processing;minimum;optimal recovery;sparse solution of linear equations},
abstract={},
doi={10.1109/TIT.2006.871582}, 
ISSN={0018-9448},}

@ARTICLE{Candes08, 
author={Candes, E.J. and Wakin, M.B.}, 
journal={Signal Processing Magazine, IEEE}, 
title={An Introduction To Compressive Sampling}, 
year=2008, 
volume=25, 
number=2, 
pages={21-30}, 
abstract={Conventional approaches to sampling signals or images follow Shannon's theorem: the sampling rate must be at least twice the maximum frequency present in the signal (Nyquist rate). In the field of data conversion, standard analog-to-digital converter (ADC) technology implements the usual quantized Shannon representation - the signal is uniformly sampled at or above the Nyquist rate. This article surveys the theory of compressive sampling, also known as compressed sensing or CS, a novel sensing/sampling paradigm that goes against the common wisdom in data acquisition. CS theory asserts that one can recover certain signals and images from far fewer samples or measurements than traditional methods use.}, 
keywords={data acquisition;image processing;signal processing equipment;signal sampling;Relatively few wavelet;compressed sensing;compressive sampling;data acquisition;image recovery;sampling paradigm;sensing paradigm;signal recovery;Biomedical imaging;Data acquisition;Frequency;Image coding;Image sampling;Protocols;Receivers;Sampling methods;Signal processing;Signal sampling}, 
doi={10.1109/MSP.2007.914731}, 
ISSN={1053-5888},}

@ARTICLE{Candes07,
author={Emmanuel CandÃ¨s and Justin Romberg},
title={Sparsity and incoherence in compressive sampling},
journal={Inverse Problems},
volume=23,
number=3,
pages=969,
year=2007,
abstract={We consider the problem of reconstructing a sparse signal $x^0\in\R^n$ from a limited number of linear measurements. Given $m$ randomly selected samples of $U x^0$, where $U$ is an orthonormal matrix, we show that $\ell_1$ minimization recovers $x^0$ exactly when the number of measurements exceeds \[ m\geq \mathrm{Const}\cdot\mu^2(U)\cdot S\cdot\log n, \] where $S$ is the number of nonzero components in $x^0$, and $\mu$ is the largest entry in $U$ properly normalized: $\mu(U) = \sqrt{n} \cdot \max_{k,j} |U_{k,j}|$. The smaller $\mu$, the fewer samples needed. \\ The result holds for ``most'' sparse signals $x^0$ supported on a fixed (but arbitrary) set $T$. Given $T$, if the sign of $x^0$ for each nonzero entry on $T$ and the observed values of $Ux^0$ are drawn at random, the signal is recovered with overwhelming probability. Moreover, there is a sense in which this is nearly optimal since any method succeeding with the same probability would require just about this many samples.},
doi={10.1088/0266-5611/23/3/008},}

@ARTICLE{Candes06,
author={Candes, E.J. and Romberg, J. and Tao, T.}, 
journal={Information Theory, IEEE Transactions on}, 
title={Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information}, 
year=2006, 
volume=52, 
number=2, 
pages={489-509}, 
abstract={This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal $f \in \C^N$ and a randomly chosen set of frequencies $\Omega$ of mean size $\tau N$. Is it possible to reconstruct $f$ from the partial knowledge of its Fourier coefficients on the set $\Omega$? \\ A typical result of this paper is as follows: for each $M > 0$, suppose that $f$ obeys $$ # \{t, f(t) \neq 0 \} \le \alpha(M) \cdot (\log N)^{-1} \cdot # \Omega, $$ then with probability at least $1-O(N^{-M})$, $f$ can be reconstructed exactly as the solution to the $\ell_1$ minimization problem $$ \min_g \sum_{t = 0}^{N-1} |g(t)|, \quad \text{s.t.} \hat g(\omega) = \hat f(\omega) \text{for all} \omega \in \Omega. $$ In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for $\alpha$ which depends on the desired probability of success; except for the logarithmic factor, the condition on the size of the support is sharp. \\ The methodology extends to a variety of other setups and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one or two-dimensional) object from incomplete frequency samples--provided that the number of jumps (discontinuities) obeys the condition above--by minimizing other convex functionals such as the total-variation of $f$.}, 
keywords={Fourier analysis;convex programming;image reconstruction;image sampling;indeterminancy;linear programming;minimisation;piecewise constant techniques;probability;signal reconstruction;signal sampling;sparse matrices;Fourier coefficient;convex optimization;discrete-time signal;image reconstruction;incomplete frequency information;linear programming;minimization problem;nonlinear sampling theorem;piecewise constant object;probability value;robust uncertainty principle;signal reconstruction;sparse random matrix;trigonometric expansion;Biomedical imaging;Frequency;Image reconstruction;Linear programming;Mathematics;Robustness;Sampling methods;Signal processing;Signal reconstruction;Uncertainty;Convex optimization;duality in optimization;free probability;image reconstruction;linear programming;random matrices;sparsity;total-variation minimization;trigonometric expansions;uncertainty principle}, 
doi={10.1109/TIT.2005.862083}, 
ISSN={0018-9448},}

@INCOLLECTION{Fornasier11,
year=2011,
isbn={978-0-387-92919-4},
booktitle={Handbook of Mathematical Methods in Imaging},
editor={Scherzer, Otmar},
doi={10.1007/978-0-387-92920-0_6},
title={Compressive Sensing},
publisher={Springer New York},
author={Fornasier, Massimo and Rauhut, Holger},
pages={187-228},}

@ARTICLE{Ji08,
author={Shihao Ji and Ya Xue and Carin, L.}, 
journal={Signal Processing, IEEE Transactions on}, 
title={Bayesian Compressive Sensing}, 
year=2008, 
volume=56, 
number=6, 
pages={2346-2356}, 
abstract={The data of interest are assumed to be represented as N-dimensional real vectors, and these vectors are compressible in some linear basis B, implying that the signal can be reconstructed accurately using only a small number M Lt N of basis-function coefficients associated with B. Compressive sensing is a framework whereby one does not measure one of the aforementioned N-dimensional signals directly, but rather a set of related measurements, with the new measurements a linear combination of the original underlying N-dimensional signal. The number of required compressive-sensing measurements is typically much smaller than N, offering the potential to simplify the sensing system. Let f denote the unknown underlying N-dimensional signal, and g a vector of compressive-sensing measurements, then one may approximate f accurately by utilizing knowledge of the (under-determined) linear relationship between f and g, in addition to knowledge of the fact that f is compressible in B. In this paper we employ a Bayesian formalism for estimating the underlying signal f based on compressive-sensing measurements g. The proposed framework has the following properties: i) in addition to estimating the underlying signal f, "error bars" are also estimated, these giving a measure of confidence in the inverted signal; ii) using knowledge of the error bars, a principled means is provided for determining when a sufficient number of compressive-sensing measurements have been performed; iii) this setting lends itself naturally to a framework whereby the compressive sensing measurements are optimized adaptively and hence not determined randomly; and iv) the framework accounts for additive noise in the compressive-sensing measurements and provides an estimate of the noise variance. In this paper we present the underlying theory, an associated algorithm, example results, and provide comparisons to other compressive-sensing inversion algorithms in the literature.}, 
keywords={Bayes methods;estimation theory;noise;signal reconstruction;signal representation;Bayesian compressive sensing framework;N-dimensional signal representation;noise variance;signal estimation;signal reconstruction;Additive noise;Bars;Bayesian methods;Design for experiments;Discrete wavelet transforms;Machine learning;Noise measurement;Performance evaluation;Transform coding;Vectors;Adaptive compressive sensing;Bayesian model selection;compressive sensing (CS);experimental design;relevance vector machine (RVM);sparse Bayesian learning}, 
doi={10.1109/TSP.2007.914345}, 
ISSN={1053-587X},}
